import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

# Generate synthetic 1D data
np.random.seed(42)
data = np.hstack([np.random.normal(0, 1, 150), np.random.normal(5, 1, 150)])

# Initialize parameters
k = 2
means = np.random.choice(data, k)
variances = np.ones(k)
weights = np.ones(k) / k

# EM algorithm
for _ in range(20):
    # E-step: compute responsibilities
    resp = np.array([w * norm.pdf(data, m, np.sqrt(v)) for w, m, v in zip(weights, means, variances)])
    resp /= resp.sum(0)

    # M-step: update parameters
    Nk = resp.sum(1)
    means = (resp @ data) / Nk
    variances = ((resp * (data - means[:, np.newaxis])**2).sum(1)) / Nk
    weights = Nk / len(data)

# Output results
print("Means:", means)
print("Variances:", variances)
print("Weights:", weights)

# Plot
x = np.linspace(min(data), max(data), 1000)
plt.hist(data, bins=30, density=True, alpha=0.5)
for i in range(k):
    plt.plot(x, weights[i] * norm.pdf(x, means[i], np.sqrt(variances[i])))
plt.title("EM for GMM")
plt.show()

output 

Iteration 1: Log-Likelihood = -1052.0648
Iteration 2: Log-Likelihood = -1031.6931
...
Iteration 11: Log-Likelihood = -995.2163

Final Means: [0.0505 5.0641]
Final Variances: [0.9428 1.0502]
Final Mixing Coefficients: [0.5 0.5]
